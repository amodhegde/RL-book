{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53684a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27f5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "from rl.chapter3.simple_inventory_mdp_cap import *\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "import numpy as np \n",
    "from pprint import pprint\n",
    "from typing import TypeVar, Iterable, Mapping, Dict, Callable, Iterator\n",
    "import matplotlib.pyplot as plt \n",
    "import rl.markov_process as mp\n",
    "import itertools\n",
    "import rl.markov_decision_process as mdp\n",
    "import rl.monte_carlo as mc\n",
    "import rl.td as td \n",
    "from rl.distribution import Choose, Categorical\n",
    "from rl.function_approx import LinearFunctionApprox, Tabular\n",
    "import rl.chapter11.control_utils as control\n",
    "import rl.iterate as iterate\n",
    "import rl.policy as policy\n",
    "from rl.approximate_dynamic_programming import QValueFunctionApprox, NTStateDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae825a",
   "metadata": {},
   "source": [
    "First finding DP value iteration optimal action and vf for simple inventory MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f697e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Policy Iteration Optimal Value Function and Optimal Policy\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.895,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.661,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.992,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.661,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.992,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.992}\n",
      "\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initializing the simple inventory mrp\n",
    "capacity = 2\n",
    "poisson_lambda = 1.0\n",
    "holding_cost = 1.0\n",
    "stockout_cost = 10.0\n",
    "gamma=0.9\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "    SimpleInventoryMDPCap(\n",
    "        capacity=capacity,\n",
    "        poisson_lambda=poisson_lambda,\n",
    "        holding_cost=holding_cost,\n",
    "        stockout_cost=stockout_cost\n",
    "    )\n",
    "\n",
    "print(\"MDP Policy Iteration Optimal Value Function and Optimal Policy\")\n",
    "\n",
    "opt_vf_pi, opt_policy_pi = value_iteration_result(si_mdp, gamma=gamma)\n",
    "\n",
    "pprint({k : round(v, 3) for k, v in opt_vf_pi.items()})\n",
    "print()\n",
    "print(opt_policy_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a65801",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "\n",
    "Finding the optimal policy and vf using SARSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c504aa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -29.624,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -23.576,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -23.785,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -24.537,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -24.869,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -25.825}\n",
      "\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ntr = 100000\n",
    "nep = 1000000\n",
    "\n",
    "approx_0 = Tabular()\n",
    "\n",
    "sarsa_qvf = td.glie_sarsa(mdp=si_mdp, states=Choose(si_mdp.non_terminal_states), approx_0=approx_0,\n",
    "                          γ=gamma, ϵ_as_func_of_episodes=lambda k : 1. / k, max_episode_length=nep)\n",
    "\n",
    "*_, qvf = itertools.islice(sarsa_qvf, ntr)\n",
    "opt_vf, opt_policy = control.get_vf_and_policy_from_qvf(mdp=si_mdp, qvf=qvf)\n",
    "\n",
    "pprint({s : round(v, 3) for s, v in opt_vf.items()})\n",
    "print()\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c5974",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "\n",
    "Finding the optimal policy and vf using MC Control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71628261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.621,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.983,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.469,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -29.063,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.463,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.347}\n",
      "\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ntr = 1000\n",
    "\n",
    "approx_0 = Tabular()\n",
    "\n",
    "mc_qvf = mc.glie_mc_control(mdp=si_mdp, states=Choose(si_mdp.non_terminal_states), approx_0=approx_0,\n",
    "                          γ=gamma, ϵ_as_func_of_episodes=lambda k : 1. / k, episode_length_tolerance = 1e-6)\n",
    "\n",
    "*_, qvf = itertools.islice(mc_qvf, ntr)\n",
    "opt_vf, opt_policy = control.get_vf_and_policy_from_qvf(mdp=si_mdp, qvf=qvf)\n",
    "\n",
    "pprint({s : round(v, 3) for s, v in opt_vf.items()})\n",
    "print()\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e2814",
   "metadata": {},
   "source": [
    "**Pending:** Extending the approx impementations to support backward iteration to solve the asset alloc problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f46cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('ml_101': conda)",
   "language": "python",
   "name": "python386jvsc74a57bd05bfd9133dd126cf38103cd89df3a296c0b5a8984bbd48b120a330d6b9578f0ac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
